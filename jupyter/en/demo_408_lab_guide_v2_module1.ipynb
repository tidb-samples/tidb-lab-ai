{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uyZcP_8TqZyG"
      },
      "source": [
        "# Module 1: RAG (Retrieval-Augmented Generation)\n",
        "\n",
        "This module uses Amazon Bedrock Embedding model and TiDB Serverless Vector Search to retrieve the data. And then, use the LLM (Large Language Model) to generate the answer of the question.\n",
        "\n",
        "Here, we demonstrate how easily you can build a RAG application using `pytidb` and TiDB Serverless."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "psEHGWiHizUq"
      },
      "source": [
        "> **Note:**\n",
        ">\n",
        "> - We already set the `SERVERLESS_CLUSTER_HOST`, `SERVERLESS_CLUSTER_PORT`, `SERVERLESS_CLUSTER_USERNAME`, `SERVERLESS_CLUSTER_PASSWORD`, and `SERVERLESS_CLUSTER_DATABASE_NAME` in the environment parameters.\n",
        "> - We also granted the permission of using Amazon Bedrock for this lab. If you want to use this code snippet out of TiDB Labs platform, please set them beforehand."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F1fsS576izUl"
      },
      "source": [
        "## Install Dependencies\n",
        "\n",
        "Click the triangular Run button on the left of the cell to run the code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pTpKX_lDizUp"
      },
      "outputs": [],
      "source": [
        "%pip install -q \\\n",
        "    pytidb==0.0.10.dev1 \\\n",
        "    boto3==1.38.23 \\\n",
        "    litellm \\\n",
        "    pandas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3WbH_BITizUr"
      },
      "source": [
        "## Initial the Clients of Database"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UWtcs58-izUr"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "from litellm import completion\n",
        "from typing import Optional, Any\n",
        "from pytidb import TiDBClient\n",
        "from pytidb.schema import TableModel, Field\n",
        "from pytidb.embeddings import EmbeddingFunction\n",
        "\n",
        "db = TiDBClient.connect(\n",
        "    host=os.getenv(\"SERVERLESS_CLUSTER_HOST\"),\n",
        "    port=int(os.getenv(\"SERVERLESS_CLUSTER_PORT\")),\n",
        "    username=os.getenv(\"SERVERLESS_CLUSTER_USERNAME\"),\n",
        "    password=os.getenv(\"SERVERLESS_CLUSTER_PASSWORD\"),\n",
        "    database=os.getenv(\"SERVERLESS_CLUSTER_DATABASE_NAME\"),\n",
        "    enable_ssl=True,\n",
        ")\n",
        "\n",
        "embedding_model = \"bedrock/amazon.titan-embed-text-v2:0\"\n",
        "\n",
        "text_embedding_function = EmbeddingFunction(\n",
        "    embedding_model,\n",
        "    timeout=60\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uOyjrmWJizUr"
      },
      "source": [
        "## Prepare the Context\n",
        "\n",
        "In this case, contexts are the documents, use the openai embeddings model to get the embeddings of the documents, and store them in the TiDB."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_e5P_m0MizUs"
      },
      "outputs": [],
      "source": [
        "table_name = \"documents\"\n",
        "class Document(TableModel, table=True):\n",
        "    __tablename__ = table_name\n",
        "    __table_args__ = {\"extend_existing\": True}\n",
        "    id: int | None = Field(default=None, primary_key=True)\n",
        "    text: str = Field(max_length=1024)\n",
        "    embedding: Optional[Any] = text_embedding_function.VectorField(\n",
        "        source_field=\"text\",\n",
        "    )\n",
        "\n",
        "documents = [\n",
        "    Document(id=1, text=\"TiDB is an open-source distributed SQL database that supports Hybrid Transactional and Analytical Processing (HTAP) workloads.\"),\n",
        "    Document(id=2, text=\"TiFlash is the key component that makes TiDB essentially an Hybrid Transactional/Analytical Processing (HTAP) database. As a columnar storage extension of TiKV, TiFlash provides both good isolation level and strong consistency guarantee.\"),\n",
        "    Document(id=3, text=\"TiKV is a distributed and transactional key-value database, which provides transactional APIs with ACID compliance. With the implementation of the Raft consensus algorithm and consensus state stored in RocksDB, TiKV guarantees data consistency between multiple replicas and high availability. \"),\n",
        "]\n",
        "\n",
        "table = db.create_table(schema=Document, if_exists=\"overwrite\")\n",
        "table.bulk_insert(documents)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "atc0gXVZizUt"
      },
      "source": [
        "## Retrieve by Cosine Distance of Vectors\n",
        "\n",
        "Get the relevant documents from the TiDB by comparing the embeddings of the question and the documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "id": "DTtJRX64izUt",
        "outputId": "3ed92d25-349a-40f8-996d-3ad8ac305c9b"
      },
      "outputs": [],
      "source": [
        "question = \"what is TiKV?\"\n",
        "\n",
        "results = table.search(question).limit(1)\n",
        "results.to_pandas()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KtEFEUkop4qx"
      },
      "source": [
        "## Generate the Answer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nZxd_RBsp6_z"
      },
      "outputs": [],
      "source": [
        "from litellm import completion\n",
        "\n",
        "llm_model = \"bedrock/us.amazon.nova-lite-v1:0\"\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": f\"Please carefully answer the question by {str(results)}\"},\n",
        "    {\"role\": \"user\", \"content\": question}\n",
        "]\n",
        "\n",
        "llm_response = completion(\n",
        "    model=llm_model,\n",
        "    messages=messages,\n",
        ")\n",
        "\n",
        "print(llm_response.choices[0].message.content)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
